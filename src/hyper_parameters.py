import torch
import os
import copy
import optuna
from importlib import import_module
from shutil import copyfile
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)

from src.validation import compute_metrics
from src.data_load import *
from src.util import label_to_num


def optuna_hp_func_with_cfg(hp_cfg):
    def hp_space_optuna(trial: optuna.trial.Trial):
        return {
            "learning_rate": trial.suggest_loguniform(
                "learning_rate", hp_cfg.learning_rate.min, hp_cfg.learning_rate.max
            ),
            "num_train_epochs": trial.suggest_int(
                "num_train_epochs",
                hp_cfg.num_train_epochs.min,
                hp_cfg.num_train_epochs.max,
            ),
            "seed": trial.suggest_int("seed", hp_cfg.seed.min, hp_cfg.seed.max),
            "warmup_steps": trial.suggest_int(
                "warmup_steps", hp_cfg.warmup_step.min, hp_cfg.warmup_step.max
            ),
            "weight_decay": trial.suggest_uniform(
                "weight_decay", hp_cfg.weight_decay.min, hp_cfg.weight_decay.max
            ),
        }

    return hp_space_optuna


def hp_space_sigopt(_):
    return [
        {
            "bounds": {"min": 5e-6, "max": 1e-4},
            "name": "learning_rate",
            "type": "double",
            "transformamtion": "log",
        },
        {"bounds": {"min": 1, "max": 6}, "name": "num_train_epochs", "type": "int"},
        {"bounds": {"min": 1, "max": 50}, "name": "seed", "type": "int"},
        {
            "categorical_values": ["4", "8", "16", "32", "64"],
            "name": "per_device_train_batch_size",
            "type": "categorical",
        },
        {"bounds": {"min": 100, "max": 1000}, "name": "warmup_steps", "type": "int"},
        {
            "bounds": {"min": 0.001, "max": 0.1},
            "name": "weight_decay",
            "type": "double",
        },
    ]


# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ìš© ì„¤ì •
def set_hp_training_args(output_dir, train_args_cfg):
    return TrainingArguments(
        output_dir=output_dir,
        evaluation_strategy=train_args_cfg.evaluation_strategy,  # evaluation strategy to adopt during training
        eval_steps=train_args_cfg.eval_steps,  # evaluation step.
        fp16=train_args_cfg.fp16,
        disable_tqdm=True,
        per_device_train_batch_size=train_args_cfg.batch_size,  # batch size per device during training
        per_device_eval_batch_size=train_args_cfg.batch_size,  # batch size for evaluation
        load_best_model_at_end=True,
        report_to="wandb",
    )


# í•˜ì´í¼ íŒŒë¼ë¯¸í„°ìš© í›ˆë ¨
def hyper_parameter_train(cfg):
    train_name = cfg.name
    print(f"\n\n *** {train_name} HyperParameter START !! ***\n\n")

    # í´ë”ìƒì„±, ì„¤ì •íŒŒì¼ ë³µì‚¬
    if not os.path.exists(cfg.dir_path.base):
        os.mkdir(cfg.dir_path.base)
    if not os.path.exists(os.path.join(cfg.dir_path.base, train_name)):
        os.mkdir(os.path.join(cfg.dir_path.base, train_name))
    copyfile(
        os.path.join(cfg.dir_path.code, "../", "main_cfg.yaml"),
        os.path.join(cfg.dir_path.base, train_name, "config.yaml"),
    )

    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    MODEL_INDEX = cfg.model.pick
    print(f"\n\n Target model: {cfg.model.model_list[MODEL_INDEX]}\n\n")
    tokenizer, model = load_tokenizer_and_model(MODEL_INDEX, cfg.model)

    # ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    train_dataset = train_data_with_addition(
        cfg.dir_path.train_data_path, cfg.dataset.add_data
    )
    temp_dataset, train_dataset = get_stratified_K_fold(
        train_dataset, cfg.hyperparameters.dataset.train
    )
    if cfg.dataset.dev_data:
        valid_dataset = train_data_with_addition(
            cfg.dir_path.dev_data_path, cfg.dataset.add_dev_data
        )
        _, valid_dataset = get_stratified_K_fold(
            valid_dataset, cfg.hyperparameters.dataset.valid
        )
    else:
        _, valid_dataset = get_stratified_K_fold(
            temp_dataset, cfg.hyperparameters.dataset.valid
        )

    print(f"\n\n train_data: {len(train_dataset)}, dev_data: {len(valid_dataset)} \n\n")

    # ë°ì´í„° ì „ì²˜ë¦¬

    print(f"\n START PreProcess \n")
    preprocess_name, is_two_sentence = cfg.preprocess.list[cfg.preprocess.pick]
    preprocess_func = getattr(import_module("src.pre_process"), preprocess_name)

    train_dataset, add_tokens, special_tokens = preprocess_func(
        train_dataset, cfg.EDA_AEDA.AEDA.set, cfg.EDA_AEDA.AEDA
    )
    train_dataset = train_dataset.sample(frac=1)  # í›ˆë ¨ë°ì´í„° ë’¤ì„ê¸°

    valid_dataset, _, _ = preprocess_func(valid_dataset)

    train_label = label_to_num(train_dataset["label"].values, cfg)
    valid_label = label_to_num(valid_dataset["label"].values, cfg)

    # ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
    print(f"\n\n tokenizing START \n\n")

    tokenizer.add_tokens(add_tokens)
    tokenizer.add_special_tokens(special_tokens)

    token_func = (
        "tokenized_dataset_with_division" if is_two_sentence else "tokenized_dataset"
    )
    tokenizer_module = getattr(import_module("src.tokenizing"), token_func)

    tokenized_train = tokenizer_module(train_dataset, tokenizer)
    tokenized_valid = tokenizer_module(valid_dataset, tokenizer)

    tokenizer.save_pretrained(
        os.path.join(cfg.dir_path.base, train_name, cfg.dir_path.tokenizer)
    )

    # íŒŒì´í† ì¹˜ ë°ì´í„°ì…‹ ì œì‘
    RE_train_dataset = RE_Dataset(tokenized_train, train_label)
    RE_valid_dataset = RE_Dataset(tokenized_valid, valid_label)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(device)

    # setting model hyperparameter
    MODEL_NAME = cfg.model.model_list[MODEL_INDEX]
    model_config = AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = cfg.model.num_labels

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, config=model_config
    )
    model.resize_token_embeddings(len(tokenizer))

    def model_init():
        return copy.deepcopy(model)

    def custom_model_init():
        return getattr(
            import_module("src.custom_models"),
            cfg.model.custom_model_args.list[cfg.model.custom_model_args.pick],
        )(cfg.model.model_list[MODEL_INDEX])

    output_dir = os.path.join(cfg.dir_path.base, train_name, "hp_optimize")

    # Training ì„¤ì •
    training_args = set_hp_training_args(output_dir, cfg.train_args)
    trainer = Trainer(
        model_init=model_init,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        tokenizer=tokenizer,
        train_dataset=RE_train_dataset,  # training dataset
        eval_dataset=RE_valid_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
    )

    # hp train model
    trainer.hyperparameter_search(
        direction="maximize",
        backend=cfg.hyperparameters.backend,
        hp_space=optuna_hp_func_with_cfg(cfg.hyperparameters.cfg),
        n_trials=cfg.hyperparameters.n_trials,
    )
