{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb938a-7f5e-4d27-aa4f-f529e9dcf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from load_data import *\n",
    "\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = [\n",
    "        'no_relation', 'org:top_members/employees', 'org:members',\n",
    "        'org:product', 'per:title', 'org:alternate_names', 'per:employee_of',\n",
    "        'org:place_of_headquarters', 'per:product',\n",
    "        'org:number_of_employees/members', 'per:children',\n",
    "        'per:place_of_residence', 'per:alternate_names', 'per:other_family',\n",
    "        'per:colleagues', 'per:origin', 'per:siblings', 'per:spouse',\n",
    "        'org:founded', 'org:political/religious_affiliation', 'org:member_of',\n",
    "        'per:parents', 'org:dissolved', 'per:schools_attended',\n",
    "        'per:date_of_death', 'per:date_of_birth', 'per:place_of_birth',\n",
    "        'per:place_of_death', 'org:founded_by', 'per:religion'\n",
    "    ]\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(\n",
    "        labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30, ))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(\n",
    "            targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validationì„ ìœ„í•œ metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = klue_re_micro_f1(preds, labels)\n",
    "    auprc = klue_re_auprc(probs, labels)\n",
    "    acc = accuracy_score(labels, preds)  # ë¦¬ë”ë³´ë“œ í‰ê°€ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "    return {\n",
    "        'micro f1 score': f1,\n",
    "        'auprc': auprc,\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label\n",
    "\n",
    "\n",
    "def train():\n",
    "    # load model and tokenizer\n",
    "    # MODEL_NAME = \"bert-base-uncased\"\n",
    "    # MODEL_NAME = \"klue/bert-base\"\n",
    "    MODEL_NAME = \"klue/roberta-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    train_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "    dev_dataset = load_data(\"../dataset/train/dev.csv\") # validationìš© ë°ì´í„°ëŠ” ë”°ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(device)\n",
    "    # setting model hyperparameter\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 30\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, config=model_config)\n",
    "    print(model.config)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    # ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "    # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',  # output directory\n",
    "        save_total_limit=5,  # number of total save model.\n",
    "        save_steps=500,  # model saving step.\n",
    "        num_train_epochs=20,  # total number of training epochs\n",
    "        learning_rate=5e-5,  # learning_rate\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./logs',  # directory for storing logs\n",
    "        logging_steps=100,  # log saving step.\n",
    "        evaluation_strategy=\n",
    "        'steps',  # evaluation strategy to adopt during training\n",
    "        # `no`: No evaluation during training.\n",
    "        # `steps`: Evaluate every `eval_steps`.\n",
    "        # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=500,  # evaluation step.\n",
    "        load_best_model_at_end=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,  # training dataset\n",
    "        eval_dataset=RE_dev_dataset,  # evaluation dataset\n",
    "        compute_metrics=compute_metrics  # define metrics function\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "    model.save_pretrained('./best_model')\n",
    "\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e58c4-a744-49d9-a843-c0db774f844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
