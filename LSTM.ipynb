{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3448de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from load_data import *\n",
    "from train import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from GPUtil import showUtilization\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78dd504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLSTM(nn.Module):\n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "        self.config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        self.config.num_labels = 30\n",
    "        self.num_labels = 30\n",
    "        \n",
    "        Model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=self.config)\n",
    "        \n",
    "        self.Bert = Model.bert        \n",
    "        self.lstm = nn.LSTM(input_size=768,\n",
    "                    hidden_size=768,\n",
    "                    num_layers=1,\n",
    "                    bidirectional=False,\n",
    "                    batch_first=True).to(device)\n",
    "        self.dropout = Model.dropout\n",
    "        self.classifier = Model.classifier\n",
    "\n",
    "        self.h_0 = torch.zeros((1, 5, 768)).to(device)  # (num_layers * num_dirs, B, d_h)\n",
    "        self.c_0 = torch.zeros((1, 5, 768)).to(device)  # (num_layers * num_dirs, B, d_h)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.Bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    " \n",
    "        lstm_outputs, _ = self.lstm(outputs[0], (self.h_0, self.c_0))\n",
    "        # pooled_output = outputs[1]\n",
    "        \n",
    "        pooled_output = self.dropout(lstm_outputs[:,-1,:])\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     output = (logits,) + outputs[2:]\n",
    "        #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        # return SequenceClassifierOutput(\n",
    "        #     loss=loss,\n",
    "        #     logits=logits,\n",
    "        #     hidden_states=outputs.hidden_states,\n",
    "        #     attentions=outputs.attentions,\n",
    "        # )\n",
    "        \n",
    "        outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "939b478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM():\n",
    "  # load model and tokenizer\n",
    "  # MODEL_NAME = \"bert-base-uncased\"\n",
    "  MODEL_NAME = \"klue/bert-base\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "  # load dataset\n",
    "  train_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "  dev_dataset = load_data(\"../dataset/train/dev.csv\") # validationÏö© Îç∞Ïù¥ÌÑ∞Îäî Îî∞Î°ú ÎßåÎìúÏÖîÏïº Ìï©ÎãàÎã§.\n",
    "\n",
    "  train_label = label_to_num(train_dataset['label'].values)\n",
    "  dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "  # tokenizing dataset\n",
    "  tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "  tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "  # make dataset for pytorch.\n",
    "  RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "  RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "  model = BertLSTM(MODEL_NAME)\n",
    "  model.to(device)\n",
    "  \n",
    "  # ÏÇ¨Ïö©Ìïú option Ïô∏ÏóêÎèÑ Îã§ÏñëÌïú optionÎì§Ïù¥ ÏûàÏäµÎãàÎã§.\n",
    "  # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments Ï∞∏Í≥†Ìï¥Ï£ºÏÑ∏Ïöî.\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir='./results/lstm_results',          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps=1500,                 # model saving step.\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    learning_rate=5e-5,               # learning_rate\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=5,   # batch size for evaluation\n",
    "    warmup_steps=1500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = 1500,            # evaluation step.\n",
    "    load_best_model_at_end = True,\n",
    "    fp16=True\n",
    "  )\n",
    "  trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=RE_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics         # define metrics function\n",
    "  )\n",
    "\n",
    "  # train model\n",
    "  trainer.train()\n",
    "  model.save_pretrained('./best_model/lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /opt/ml/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.7cee10e8ea7ffa278f8be4b141000263f2b18795e5ef5e025352b2af6851f8fb\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 32470\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 129880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42886' max='129880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42886/129880 1:49:54 < 3:42:57, 6.50 it/s, Epoch 6.60/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro f1 score</th>\n",
       "      <th>Auprc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.969600</td>\n",
       "      <td>1.179118</td>\n",
       "      <td>48.086754</td>\n",
       "      <td>35.783949</td>\n",
       "      <td>0.640695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.885300</td>\n",
       "      <td>1.241839</td>\n",
       "      <td>46.490736</td>\n",
       "      <td>44.774888</td>\n",
       "      <td>0.601159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.886200</td>\n",
       "      <td>1.048494</td>\n",
       "      <td>56.156034</td>\n",
       "      <td>50.091493</td>\n",
       "      <td>0.649324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1.092538</td>\n",
       "      <td>54.634994</td>\n",
       "      <td>50.247818</td>\n",
       "      <td>0.659111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>1.176207</td>\n",
       "      <td>54.924299</td>\n",
       "      <td>50.067743</td>\n",
       "      <td>0.679717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>1.038631</td>\n",
       "      <td>56.571516</td>\n",
       "      <td>54.599740</td>\n",
       "      <td>0.696330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>1.181854</td>\n",
       "      <td>56.548491</td>\n",
       "      <td>51.429074</td>\n",
       "      <td>0.679845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.188906</td>\n",
       "      <td>53.546652</td>\n",
       "      <td>52.944060</td>\n",
       "      <td>0.653703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>1.340549</td>\n",
       "      <td>53.825484</td>\n",
       "      <td>52.996856</td>\n",
       "      <td>0.651127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>1.318020</td>\n",
       "      <td>54.427037</td>\n",
       "      <td>50.443517</td>\n",
       "      <td>0.712814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>1.222066</td>\n",
       "      <td>56.023272</td>\n",
       "      <td>50.848927</td>\n",
       "      <td>0.715132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.719900</td>\n",
       "      <td>1.330143</td>\n",
       "      <td>54.253563</td>\n",
       "      <td>53.322185</td>\n",
       "      <td>0.673535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>1.403246</td>\n",
       "      <td>56.542597</td>\n",
       "      <td>52.793429</td>\n",
       "      <td>0.715518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>1.558847</td>\n",
       "      <td>55.433670</td>\n",
       "      <td>53.885989</td>\n",
       "      <td>0.675209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>1.480688</td>\n",
       "      <td>53.546099</td>\n",
       "      <td>53.225325</td>\n",
       "      <td>0.667740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>1.652509</td>\n",
       "      <td>53.667166</td>\n",
       "      <td>51.668812</td>\n",
       "      <td>0.639665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>1.429205</td>\n",
       "      <td>54.919137</td>\n",
       "      <td>53.237769</td>\n",
       "      <td>0.708178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>1.677597</td>\n",
       "      <td>53.960018</td>\n",
       "      <td>51.632787</td>\n",
       "      <td>0.682164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>1.823269</td>\n",
       "      <td>53.531599</td>\n",
       "      <td>48.463634</td>\n",
       "      <td>0.670058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>1.849630</td>\n",
       "      <td>52.299169</td>\n",
       "      <td>48.636313</td>\n",
       "      <td>0.632840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>1.649316</td>\n",
       "      <td>54.684964</td>\n",
       "      <td>54.408706</td>\n",
       "      <td>0.670702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.380800</td>\n",
       "      <td>1.878446</td>\n",
       "      <td>50.703417</td>\n",
       "      <td>52.628569</td>\n",
       "      <td>0.640180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>1.586239</td>\n",
       "      <td>56.279914</td>\n",
       "      <td>52.350392</td>\n",
       "      <td>0.706504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>1.796460</td>\n",
       "      <td>56.695799</td>\n",
       "      <td>54.234305</td>\n",
       "      <td>0.668384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>1.779581</td>\n",
       "      <td>55.611672</td>\n",
       "      <td>51.338720</td>\n",
       "      <td>0.661816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>1.888587</td>\n",
       "      <td>55.669236</td>\n",
       "      <td>51.948237</td>\n",
       "      <td>0.658725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>1.816796</td>\n",
       "      <td>57.374982</td>\n",
       "      <td>52.911399</td>\n",
       "      <td>0.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>1.945296</td>\n",
       "      <td>56.568287</td>\n",
       "      <td>52.278809</td>\n",
       "      <td>0.676626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-4500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-6000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-7500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-9000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-10500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-12000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-13500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-15000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-7500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-16500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-10500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-18000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-19500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-13500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-21000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-22500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-16500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-24000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-25500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-19500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-27000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-21000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-28500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-22500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-30000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-31500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-25500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-33000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-27000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-34500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-28500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-36000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-37500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-31500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-39000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-40500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-34500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7765\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to ./results/lstm_results/checkpoint-42000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [results/lstm_results/checkpoint-36000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f08778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
